{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8806170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DownSamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=15):\n",
    "        super().__init__()\n",
    "        # Convolution avec stride=2 pour réduire la taille par 2 (équivalent décimage intelligent)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=2, \n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.act(self.conv(x)))\n",
    "\n",
    "class UpSamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
    "        super().__init__()\n",
    "        # Interpolation linéaire pour agrandir + Convolution\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=1, \n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        # 1. Upsample (Interpolation linéaire)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='linear', align_corners=False)\n",
    "        \n",
    "        # 2. Gestion des petits décalages de taille (padding)\n",
    "        if x.shape[-1] != skip_connection.shape[-1]:\n",
    "            diff = skip_connection.shape[-1] - x.shape[-1]\n",
    "            x = F.pad(x, (0, diff))\n",
    "            \n",
    "        # 3. Concaténation (Skip Connection)\n",
    "        x = torch.cat([x, skip_connection], dim=1)\n",
    "        \n",
    "        # 4. Convolution finale\n",
    "        return self.bn(self.act(self.conv(x)))\n",
    "\n",
    "class WaveUNet(nn.Module):\n",
    "    def __init__(self, num_levels=4, base_channels=24):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.levels = num_levels\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        \n",
    "        # --- Encoder (Descente) ---\n",
    "        in_ch = 1 \n",
    "        out_ch = base_channels\n",
    "        self.skip_channels_history = []\n",
    "        \n",
    "        for _ in range(num_levels):\n",
    "            self.down_blocks.append(DownSamplingBlock(in_ch, out_ch))\n",
    "            self.skip_channels_history.append(in_ch)\n",
    "            in_ch = out_ch\n",
    "            out_ch *= 2 \n",
    "            \n",
    "        # --- Bottleneck ---\n",
    "        self.bottleneck = nn.Conv1d(in_ch, out_ch, kernel_size=15, stride=1, padding=7)\n",
    "        self.bottleneck_act = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        # --- Decoder (Remontée) ---\n",
    "        in_ch = out_ch \n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            # On récupère la taille du skip\n",
    "            skip_ch = self.skip_channels_history[-(i+1)]\n",
    "            \n",
    "            # --- CORRECTION ICI ---\n",
    "            if i == num_levels - 1:\n",
    "                # Si c'est le DERNIER bloc, on ne veut pas redescendre à 1 canal.\n",
    "                # On veut sortir 'base_channels' (24) pour nourrir la final_conv.\n",
    "                out_ch = base_channels\n",
    "            else:\n",
    "                # Sinon, on redescend normalement à la taille du skip\n",
    "                out_ch = skip_ch\n",
    "            \n",
    "            self.up_blocks.append(UpSamplingBlock(in_ch + skip_ch, out_ch))\n",
    "            in_ch = out_ch\n",
    "            \n",
    "        # --- Sortie ---\n",
    "        # Attend base_channels (24) en entrée\n",
    "        self.final_conv = nn.Conv1d(base_channels, 2, kernel_size=1, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        \n",
    "        # Encoder\n",
    "        for block in self.down_blocks:\n",
    "            skips.append(x)\n",
    "            x = block(x)\n",
    "            \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck_act(self.bottleneck(x))\n",
    "        \n",
    "        # Decoder\n",
    "        for i, block in enumerate(self.up_blocks):\n",
    "            skip = skips[-(i+1)]\n",
    "            x = block(x, skip)\n",
    "            \n",
    "        out = torch.tanh(self.final_conv(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8958560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, base_dir, sample_rate=16000, segment_length=16384):\n",
    "        self.base_dir = base_dir\n",
    "        self.sr = sample_rate\n",
    "        self.segment_length = segment_length\n",
    "        # On récupère tous les dossiers 0001, 0002...\n",
    "        self.folders = sorted(glob.glob(os.path.join(base_dir, \"*\")))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.folders)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        folder = self.folders[idx]\n",
    "        \n",
    "        # 1. Charger les fichiers\n",
    "        # On cherche le fichier mix (peu importe le SNR)\n",
    "        mix_path = glob.glob(os.path.join(folder, \"mix_snr_*.wav\"))[0]\n",
    "        voice_path = os.path.join(folder, \"voice.wav\")\n",
    "        noise_path = os.path.join(folder, \"noise.wav\")\n",
    "        \n",
    "        mix, _ = librosa.load(mix_path, sr=self.sr)\n",
    "        voice, _ = librosa.load(voice_path, sr=self.sr)\n",
    "        noise, _ = librosa.load(noise_path, sr=self.sr)\n",
    "        \n",
    "        # 2. Découpage aléatoire (Random Crop)\n",
    "        # On prend un extrait de 'segment_length'\n",
    "        if len(mix) > self.segment_length:\n",
    "            start = random.randint(0, len(mix) - self.segment_length)\n",
    "            end = start + self.segment_length\n",
    "            mix = mix[start:end]\n",
    "            voice = voice[start:end]\n",
    "            noise = noise[start:end]\n",
    "        else:\n",
    "            # Si trop court, on pad avec des zéros\n",
    "            pad_len = self.segment_length - len(mix)\n",
    "            mix = np.pad(mix, (0, pad_len))\n",
    "            voice = np.pad(voice, (0, pad_len))\n",
    "            noise = np.pad(noise, (0, pad_len))\n",
    "            \n",
    "        # 3. Format PyTorch (Channel, Time) -> (1, T)\n",
    "        mix = torch.FloatTensor(mix).unsqueeze(0)\n",
    "        # Target: (2, T) -> Canal 0: Voix, Canal 1: Bruit\n",
    "        targets = np.stack([voice, noise])\n",
    "        targets = torch.FloatTensor(targets)\n",
    "        \n",
    "        return mix, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7217abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd94146623e24d50937b26ebbc9b2424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vadim\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\vadim\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "c:\\Users\\vadim\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train: 0.0339 | Loss Val: 0.0147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4f0b9bb6ab458b95052f2874ae1905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train: 0.0088 | Loss Val: 0.0055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f3b03793cf4afa84d889fad135c8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train: 0.0054 | Loss Val: 0.0045\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train_wave_unet(train_loader, val_loader, epochs=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = WaveUNet(num_levels=5, base_channels=24).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.L1Loss() # MAE est souvent meilleur que MSE pour l'audio brut\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss = 0\n",
    "        \n",
    "        for mix, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            mix, target = mix.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(mix) # Output shape: (Batch, 2, Time)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "        train_losses.append(batch_loss / len(train_loader))\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for mix, target in val_loader:\n",
    "                mix, target = mix.to(device), target.to(device)\n",
    "                output = model(mix)\n",
    "                val_loss += criterion(output, target).item()\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Loss Train: {train_losses[-1]:.4f} | Loss Val: {val_losses[-1]:.4f}\")\n",
    "        \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# --- LANCEMENT ---\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 1. Création du Dataset complet\n",
    "full_train_dataset = WaveformDataset(\"train/\", sample_rate=16000)\n",
    "\n",
    "# 2. Calcul des tailles (ex: 80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "# 3. Split aléatoire\n",
    "train_subset, val_subset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# 4. DataLoaders séparés\n",
    "dataloader_train = DataLoader(train_subset, batch_size=3, shuffle=True)\n",
    "dataloader_val = DataLoader(val_subset, batch_size=3, shuffle=False)\n",
    "\n",
    "# Maintenant vous pouvez lancer train_wave_unet(dataloader_train, dataloader_val, ...)\n",
    "\n",
    "model, t_loss, v_loss = train_wave_unet(dataloader_train, dataloader_val, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5933218d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score, voice_est, sr\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation après entraînement\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m score, wav_out, sr \u001b[38;5;241m=\u001b[39m evaluate_wave_unet(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/0001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSI-SDR Wave-U-Net : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_si_sdr(reference, estimation):\n",
    "    min_len = min(len(reference), len(estimation))\n",
    "    reference = reference[:min_len]\n",
    "    estimation = estimation[:min_len]\n",
    "    dot_product = np.dot(reference, estimation)\n",
    "    norm_ref = np.linalg.norm(reference)**2\n",
    "    projection = (dot_product / (norm_ref + 1e-8)) * reference\n",
    "    noise = estimation - projection\n",
    "    si_sdr = 10 * np.log10(np.linalg.norm(projection)**2 / (np.linalg.norm(noise)**2 + 1e-8))\n",
    "    return si_sdr\n",
    "\n",
    "def evaluate_wave_unet(model, test_folder):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    mix_path = glob.glob(os.path.join(test_folder, \"mix_snr_*.wav\"))[0]\n",
    "    voice_path = os.path.join(test_folder, \"voice.wav\")\n",
    "    \n",
    "    # Chargement\n",
    "    mix, sr = librosa.load(mix_path, sr=16000)\n",
    "    voice_true, _ = librosa.load(voice_path, sr=16000)\n",
    "    \n",
    "    # Préparation\n",
    "    input_tensor = torch.FloatTensor(mix).unsqueeze(0).unsqueeze(0).to(device) # (1, 1, T)\n",
    "    \n",
    "    # Inférence (On coupe le gradient pour sauver la RAM)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "        \n",
    "    # Récupération (Canal 0 = Voix)\n",
    "    voice_est = prediction[0, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Calcul SI-SDR\n",
    "    score = calculate_si_sdr(voice_true, voice_est)\n",
    "    return score, voice_est, sr\n",
    "\n",
    "# Exemple d'utilisation après entraînement\n",
    "score, wav_out, sr = evaluate_wave_unet(model, \"train/0001\")\n",
    "print(f\"SI-SDR Wave-U-Net : {score:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def calculate_si_sdr(reference, estimation):\n",
    "    \"\"\"\n",
    "    Calcule le SI-SDR (Scale-Invariant Signal-to-Distortion Ratio)\n",
    "    \"\"\"\n",
    "    # Alignement des tailles (par sécurité)\n",
    "    min_len = min(len(reference), len(estimation))\n",
    "    reference = reference[:min_len]\n",
    "    estimation = estimation[:min_len]\n",
    "    \n",
    "    # Éviter la division par zéro\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # Projection du signal estimé sur la référence\n",
    "    dot_product = np.dot(reference, estimation)\n",
    "    norm_ref = np.linalg.norm(reference)**2\n",
    "    projection = (dot_product / (norm_ref + eps)) * reference\n",
    "    \n",
    "    # Le bruit est la partie orthogonale\n",
    "    noise = estimation - projection\n",
    "    \n",
    "    # Calcul du ratio en dB\n",
    "    numerator = np.linalg.norm(projection)**2\n",
    "    denominator = np.linalg.norm(noise)**2\n",
    "    si_sdr = 10 * np.log10(numerator / (denominator + eps))\n",
    "    \n",
    "    return si_sdr\n",
    "\n",
    "def evaluate_on_test_set(model, test_root_dir=\"test/\"):\n",
    "    \"\"\"\n",
    "    Évalue le modèle sur TOUS les dossiers contenus dans test_root_dir.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Récupérer tous les sous-dossiers (0001, 0002, etc.) dans 'test/'\n",
    "    test_folders = sorted(glob.glob(os.path.join(test_root_dir, \"*\")))\n",
    "    \n",
    "    if len(test_folders) == 0:\n",
    "        print(f\"Attention : Aucun dossier trouvé dans {test_root_dir}\")\n",
    "        return\n",
    "        \n",
    "    si_sdr_scores = []\n",
    "    \n",
    "    print(f\"Début de l'évaluation sur {len(test_folders)} fichiers de test...\")\n",
    "    \n",
    "    with torch.no_grad(): # Pas de gradient pour l'inférence (économie mémoire)\n",
    "        for folder in tqdm(test_folders):\n",
    "            # 1. Trouver les fichiers\n",
    "            mix_files = glob.glob(os.path.join(folder, \"mix_snr_*.wav\"))\n",
    "            if not mix_files:\n",
    "                continue # On saute si dossier vide\n",
    "                \n",
    "            mix_path = mix_files[0]\n",
    "            voice_path = os.path.join(folder, \"voice.wav\")\n",
    "            \n",
    "            # 2. Chargement Audio\n",
    "            # On charge tout le fichier (sr=16000 pour Wave-U-Net)\n",
    "            mix, sr = librosa.load(mix_path, sr=16000)\n",
    "            voice_true, _ = librosa.load(voice_path, sr=16000)\n",
    "            \n",
    "            # 3. Préparation Tenseur\n",
    "            # Shape: (Batch=1, Channel=1, Time=N)\n",
    "            input_tensor = torch.FloatTensor(mix).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 4. Prédiction Wave-U-Net\n",
    "            prediction = model(input_tensor)\n",
    "            \n",
    "            # 5. Récupération Voix (Canal 0)\n",
    "            # On repasse sur CPU et en Numpy\n",
    "            voice_est = prediction[0, 0, :].cpu().numpy()\n",
    "            \n",
    "            # 6. Score\n",
    "            score = calculate_si_sdr(voice_true, voice_est)\n",
    "            si_sdr_scores.append(score)\n",
    "\n",
    "    # Statistiques Finales\n",
    "    mean_score = np.mean(si_sdr_scores)\n",
    "    median_score = np.median(si_sdr_scores)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"RÉSULTATS SUR LE JEU DE TEST ({len(si_sdr_scores)} fichiers)\")\n",
    "    print(f\"SI-SDR Moyen   : {mean_score:.2f} dB\")\n",
    "    print(f\"SI-SDR Médian  : {median_score:.2f} dB\")\n",
    "    print(f\"SI-SDR Min     : {np.min(si_sdr_scores):.2f} dB\")\n",
    "    print(f\"SI-SDR Max     : {np.max(si_sdr_scores):.2f} dB\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return si_sdr_scores\n",
    "\n",
    "# --- LANCEMENT DE L'ÉVALUATION ---\n",
    "# Assurez-vous que votre modèle est entraîné avant de lancer ceci !\n",
    "scores = evaluate_on_test_set(model, test_root_dir=\"test/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
