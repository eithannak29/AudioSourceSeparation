{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8806170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DownSamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=15):\n",
    "        super().__init__()\n",
    "        # Convolution avec stride=2 pour réduire la taille par 2 (équivalent décimage intelligent)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=2, \n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.act(self.conv(x)))\n",
    "\n",
    "class UpSamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
    "        super().__init__()\n",
    "        # Interpolation linéaire pour agrandir + Convolution\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=1, \n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        # 1. Upsample (Interpolation linéaire)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='linear', align_corners=False)\n",
    "        \n",
    "        # 2. Gestion des petits décalages de taille (padding)\n",
    "        if x.shape[-1] != skip_connection.shape[-1]:\n",
    "            diff = skip_connection.shape[-1] - x.shape[-1]\n",
    "            x = F.pad(x, (0, diff))\n",
    "            \n",
    "        # 3. Concaténation (Skip Connection)\n",
    "        x = torch.cat([x, skip_connection], dim=1)\n",
    "        \n",
    "        # 4. Convolution finale\n",
    "        return self.bn(self.act(self.conv(x)))\n",
    "\n",
    "class WaveUNet(nn.Module):\n",
    "    def __init__(self, num_levels=4, base_channels=24):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.levels = num_levels\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        \n",
    "        # --- Encoder (Descente) ---\n",
    "        in_ch = 1 \n",
    "        out_ch = base_channels\n",
    "        self.skip_channels_history = []\n",
    "        \n",
    "        for _ in range(num_levels):\n",
    "            self.down_blocks.append(DownSamplingBlock(in_ch, out_ch))\n",
    "            self.skip_channels_history.append(in_ch)\n",
    "            in_ch = out_ch\n",
    "            out_ch *= 2 \n",
    "            \n",
    "        # --- Bottleneck ---\n",
    "        self.bottleneck = nn.Conv1d(in_ch, out_ch, kernel_size=15, stride=1, padding=7)\n",
    "        self.bottleneck_act = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        # --- Decoder (Remontée) ---\n",
    "        in_ch = out_ch \n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            # On récupère la taille du skip\n",
    "            skip_ch = self.skip_channels_history[-(i+1)]\n",
    "            \n",
    "            # --- CORRECTION ICI ---\n",
    "            if i == num_levels - 1:\n",
    "                # Si c'est le DERNIER bloc, on ne veut pas redescendre à 1 canal.\n",
    "                # On veut sortir 'base_channels' (24) pour nourrir la final_conv.\n",
    "                out_ch = base_channels\n",
    "            else:\n",
    "                # Sinon, on redescend normalement à la taille du skip\n",
    "                out_ch = skip_ch\n",
    "            \n",
    "            self.up_blocks.append(UpSamplingBlock(in_ch + skip_ch, out_ch))\n",
    "            in_ch = out_ch\n",
    "            \n",
    "        # --- Sortie ---\n",
    "        # Attend base_channels (24) en entrée\n",
    "        self.final_conv = nn.Conv1d(base_channels, 2, kernel_size=1, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        \n",
    "        # Encoder\n",
    "        for block in self.down_blocks:\n",
    "            skips.append(x)\n",
    "            x = block(x)\n",
    "            \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck_act(self.bottleneck(x))\n",
    "        \n",
    "        # Decoder\n",
    "        for i, block in enumerate(self.up_blocks):\n",
    "            skip = skips[-(i+1)]\n",
    "            x = block(x, skip)\n",
    "            \n",
    "        out = torch.tanh(self.final_conv(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8958560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eithan/Desktop/AudioSourceSeparation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, base_dir, sample_rate=16000, segment_length=16384):\n",
    "        self.base_dir = base_dir\n",
    "        self.sr = sample_rate\n",
    "        self.segment_length = segment_length\n",
    "        # On récupère tous les dossiers 0001, 0002...\n",
    "        self.folders = sorted(glob.glob(os.path.join(base_dir, \"*\")))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.folders)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        folder = self.folders[idx]\n",
    "        \n",
    "        # 1. Charger les fichiers\n",
    "        # On cherche le fichier mix (peu importe le SNR)\n",
    "        mix_path = glob.glob(os.path.join(folder, \"mix_snr_*.wav\"))[0]\n",
    "        voice_path = os.path.join(folder, \"voice.wav\")\n",
    "        noise_path = os.path.join(folder, \"noise.wav\")\n",
    "        \n",
    "        mix, _ = librosa.load(mix_path, sr=self.sr)\n",
    "        voice, _ = librosa.load(voice_path, sr=self.sr)\n",
    "        noise, _ = librosa.load(noise_path, sr=self.sr)\n",
    "        \n",
    "        # 2. Découpage aléatoire (Random Crop)\n",
    "        # On prend un extrait de 'segment_length'\n",
    "        if len(mix) > self.segment_length:\n",
    "            start = random.randint(0, len(mix) - self.segment_length)\n",
    "            end = start + self.segment_length\n",
    "            mix = mix[start:end]\n",
    "            voice = voice[start:end]\n",
    "            noise = noise[start:end]\n",
    "        else:\n",
    "            # Si trop court, on pad avec des zéros\n",
    "            pad_len = self.segment_length - len(mix)\n",
    "            mix = np.pad(mix, (0, pad_len))\n",
    "            voice = np.pad(voice, (0, pad_len))\n",
    "            noise = np.pad(noise, (0, pad_len))\n",
    "            \n",
    "        # 3. Format PyTorch (Channel, Time) -> (1, T)\n",
    "        mix = torch.FloatTensor(mix).unsqueeze(0)\n",
    "        # Target: (2, T) -> Canal 0: Voix, Canal 1: Bruit\n",
    "        targets = np.stack([voice, noise])\n",
    "        targets = torch.FloatTensor(targets)\n",
    "        \n",
    "        return mix, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7217abf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m train_subset, val_subset = random_split(full_train_dataset, [train_size, val_size])\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# 4. DataLoaders séparés\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m dataloader_train = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m dataloader_val = DataLoader(val_subset, batch_size=\u001b[32m3\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Maintenant vous pouvez lancer train_wave_unet(dataloader_train, dataloader_val, ...)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AudioSourceSeparation/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:388\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    390\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AudioSourceSeparation/.venv/lib/python3.11/site-packages/torch/utils/data/sampler.py:162\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    158\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m     )\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "def train_wave_unet(train_loader, val_loader, epochs=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = WaveUNet(num_levels=5, base_channels=24).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.L1Loss() # MAE est souvent meilleur que MSE pour l'audio brut\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss = 0\n",
    "        \n",
    "        for mix, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            mix, target = mix.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(mix) # Output shape: (Batch, 2, Time)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "        train_losses.append(batch_loss / len(train_loader))\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for mix, target in val_loader:\n",
    "                mix, target = mix.to(device), target.to(device)\n",
    "                output = model(mix)\n",
    "                val_loss += criterion(output, target).item()\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Loss Train: {train_losses[-1]:.4f} | Loss Val: {val_losses[-1]:.4f}\")\n",
    "        \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# --- LANCEMENT ---\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 1. Création du Dataset complet\n",
    "full_train_dataset = WaveformDataset(\"train/\", sample_rate=16000)\n",
    "\n",
    "# 2. Calcul des tailles (ex: 80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "# 3. Split aléatoire\n",
    "train_subset, val_subset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# 4. DataLoaders séparés\n",
    "dataloader_train = DataLoader(train_subset, batch_size=3, shuffle=True)\n",
    "dataloader_val = DataLoader(val_subset, batch_size=3, shuffle=False)\n",
    "\n",
    "# Maintenant vous pouvez lancer train_wave_unet(dataloader_train, dataloader_val, ...)\n",
    "\n",
    "model, t_loss, v_loss = train_wave_unet(dataloader_train, dataloader_val, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5933218d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score, voice_est, sr\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Exemple d'utilisation après entraînement\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m score, wav_out, sr = evaluate_wave_unet(\u001b[43mmodel\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mtrain/0001\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSI-SDR Wave-U-Net : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_si_sdr(reference, estimation):\n",
    "    min_len = min(len(reference), len(estimation))\n",
    "    reference = reference[:min_len]\n",
    "    estimation = estimation[:min_len]\n",
    "    dot_product = np.dot(reference, estimation)\n",
    "    norm_ref = np.linalg.norm(reference)**2\n",
    "    projection = (dot_product / (norm_ref + 1e-8)) * reference\n",
    "    noise = estimation - projection\n",
    "    si_sdr = 10 * np.log10(np.linalg.norm(projection)**2 / (np.linalg.norm(noise)**2 + 1e-8))\n",
    "    return si_sdr\n",
    "\n",
    "def evaluate_wave_unet(model, test_folder):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    mix_path = glob.glob(os.path.join(test_folder, \"mix_snr_*.wav\"))[0]\n",
    "    voice_path = os.path.join(test_folder, \"voice.wav\")\n",
    "    \n",
    "    # Chargement\n",
    "    mix, sr = librosa.load(mix_path, sr=16000)\n",
    "    voice_true, _ = librosa.load(voice_path, sr=16000)\n",
    "    \n",
    "    # Préparation\n",
    "    input_tensor = torch.FloatTensor(mix).unsqueeze(0).unsqueeze(0).to(device) # (1, 1, T)\n",
    "    \n",
    "    # Inférence (On coupe le gradient pour sauver la RAM)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "        \n",
    "    # Récupération (Canal 0 = Voix)\n",
    "    voice_est = prediction[0, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Calcul SI-SDR\n",
    "    score = calculate_si_sdr(voice_true, voice_est)\n",
    "    return score, voice_est, sr\n",
    "\n",
    "# Exemple d'utilisation après entraînement\n",
    "score, wav_out, sr = evaluate_wave_unet(model, \"train/0001\")\n",
    "print(f\"SI-SDR Wave-U-Net : {score:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def calculate_si_sdr(reference, estimation):\n",
    "    \"\"\"\n",
    "    Calcule le SI-SDR (Scale-Invariant Signal-to-Distortion Ratio)\n",
    "    \"\"\"\n",
    "    # Alignement des tailles (par sécurité)\n",
    "    min_len = min(len(reference), len(estimation))\n",
    "    reference = reference[:min_len]\n",
    "    estimation = estimation[:min_len]\n",
    "    \n",
    "    # Éviter la division par zéro\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # Projection du signal estimé sur la référence\n",
    "    dot_product = np.dot(reference, estimation)\n",
    "    norm_ref = np.linalg.norm(reference)**2\n",
    "    projection = (dot_product / (norm_ref + eps)) * reference\n",
    "    \n",
    "    # Le bruit est la partie orthogonale\n",
    "    noise = estimation - projection\n",
    "    \n",
    "    # Calcul du ratio en dB\n",
    "    numerator = np.linalg.norm(projection)**2\n",
    "    denominator = np.linalg.norm(noise)**2\n",
    "    si_sdr = 10 * np.log10(numerator / (denominator + eps))\n",
    "    \n",
    "    return si_sdr\n",
    "\n",
    "def evaluate_on_test_set(model, test_root_dir=\"test/\"):\n",
    "    \"\"\"\n",
    "    Évalue le modèle sur TOUS les dossiers contenus dans test_root_dir.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Récupérer tous les sous-dossiers (0001, 0002, etc.) dans 'test/'\n",
    "    test_folders = sorted(glob.glob(os.path.join(test_root_dir, \"*\")))\n",
    "    \n",
    "    if len(test_folders) == 0:\n",
    "        print(f\"Attention : Aucun dossier trouvé dans {test_root_dir}\")\n",
    "        return\n",
    "        \n",
    "    si_sdr_scores = []\n",
    "    \n",
    "    print(f\"Début de l'évaluation sur {len(test_folders)} fichiers de test...\")\n",
    "    \n",
    "    with torch.no_grad(): # Pas de gradient pour l'inférence (économie mémoire)\n",
    "        for folder in tqdm(test_folders):\n",
    "            # 1. Trouver les fichiers\n",
    "            mix_files = glob.glob(os.path.join(folder, \"mix_snr_*.wav\"))\n",
    "            if not mix_files:\n",
    "                continue # On saute si dossier vide\n",
    "                \n",
    "            mix_path = mix_files[0]\n",
    "            voice_path = os.path.join(folder, \"voice.wav\")\n",
    "            \n",
    "            # 2. Chargement Audio\n",
    "            # On charge tout le fichier (sr=16000 pour Wave-U-Net)\n",
    "            mix, sr = librosa.load(mix_path, sr=16000)\n",
    "            voice_true, _ = librosa.load(voice_path, sr=16000)\n",
    "            \n",
    "            # 3. Préparation Tenseur\n",
    "            # Shape: (Batch=1, Channel=1, Time=N)\n",
    "            input_tensor = torch.FloatTensor(mix).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 4. Prédiction Wave-U-Net\n",
    "            prediction = model(input_tensor)\n",
    "            \n",
    "            # 5. Récupération Voix (Canal 0)\n",
    "            # On repasse sur CPU et en Numpy\n",
    "            voice_est = prediction[0, 0, :].cpu().numpy()\n",
    "            \n",
    "            # 6. Score\n",
    "            score = calculate_si_sdr(voice_true, voice_est)\n",
    "            si_sdr_scores.append(score)\n",
    "\n",
    "    # Statistiques Finales\n",
    "    mean_score = np.mean(si_sdr_scores)\n",
    "    median_score = np.median(si_sdr_scores)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"RÉSULTATS SUR LE JEU DE TEST ({len(si_sdr_scores)} fichiers)\")\n",
    "    print(f\"SI-SDR Moyen   : {mean_score:.2f} dB\")\n",
    "    print(f\"SI-SDR Médian  : {median_score:.2f} dB\")\n",
    "    print(f\"SI-SDR Min     : {np.min(si_sdr_scores):.2f} dB\")\n",
    "    print(f\"SI-SDR Max     : {np.max(si_sdr_scores):.2f} dB\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return si_sdr_scores\n",
    "\n",
    "# --- LANCEMENT DE L'ÉVALUATION ---\n",
    "# Assurez-vous que votre modèle est entraîné avant de lancer ceci !\n",
    "scores = evaluate_on_test_set(model, test_root_dir=\"test/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiosourceseparation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
